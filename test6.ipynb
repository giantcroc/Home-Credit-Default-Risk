{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold,cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "sns.set_style('darkgrid')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout, BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.models import load_model\n",
    "import gc\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data and test\n",
      "data test Shapes :  (307511, 122) (48744, 121)\n",
      "all_data Shape :  (356255, 122)\n",
      "['NAME_CONTRACT_TYPE', 'NAME_TYPE_SUITE', 'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'OCCUPATION_TYPE', 'WEEKDAY_APPR_PROCESS_START', 'ORGANIZATION_TYPE', 'FONDKAPREMONT_MODE', 'HOUSETYPE_MODE', 'WALLSMATERIAL_MODE', 'EMERGENCYSTATE_MODE']\n",
      "all_data Shape :  (356255, 255)\n",
      "all_data Shape :  (356255, 261)\n",
      "bureau  buro_bal Shape :  (1716428, 17) (27299925, 3)\n",
      "['STATUS']\n",
      "['CREDIT_ACTIVE', 'CREDIT_CURRENCY', 'CREDIT_TYPE']\n",
      "bb_agg Shape :  (817395, 13)\n",
      "bureau Shape :  (1716428, 52)\n",
      "bureau_agg Shape :  (305811, 71)\n",
      "bureau_agg Shape :  (305811, 107)\n",
      "bureau_agg Shape :  (305811, 145)\n",
      "all_data Shape :  (356255, 405)\n",
      "prev Shape :  (1670214, 37)\n",
      "['NAME_CONTRACT_TYPE', 'WEEKDAY_APPR_PROCESS_START', 'FLAG_LAST_APPL_PER_CONTRACT', 'NAME_CASH_LOAN_PURPOSE', 'NAME_CONTRACT_STATUS', 'NAME_PAYMENT_TYPE', 'CODE_REJECT_REASON', 'NAME_TYPE_SUITE', 'NAME_CLIENT_TYPE', 'NAME_GOODS_CATEGORY', 'NAME_PORTFOLIO', 'NAME_PRODUCT_TYPE', 'CHANNEL_TYPE', 'NAME_SELLER_INDUSTRY', 'NAME_YIELD_GROUP', 'PRODUCT_COMBINATION']\n",
      "prev_agg Shape :  (338857, 212)\n",
      "prev_agg Shape :  (338857, 266)\n",
      "prev_agg Shape :  (338857, 319)\n",
      "all_data Shape :  (356255, 723)\n",
      "pos Shape :  (10001358, 8)\n",
      "['NAME_CONTRACT_STATUS']\n",
      "pos_agg Shape :  (337252, 26)\n",
      "all_data Shape :  (356255, 749)\n",
      "ins Shape :  (13605401, 8)\n",
      "[]\n",
      "ins_agg Shape :  (339587, 35)\n",
      "all_data Shape :  (356255, 784)\n",
      "cc Shape :  (3840312, 23)\n",
      "['NAME_CONTRACT_STATUS']\n",
      "cc_agg Shape :  (103558, 81)\n",
      "all_data Shape :  (356255, 865)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def one_hot_encoder(df, nan_as_category = True):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    print (categorical_columns)\n",
    "    df = pd.get_dummies(df, columns= categorical_columns, dummy_na= nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns\n",
    "\n",
    "print('Read data and test')\n",
    "data = pd.read_csv('./input/application_train.csv')\n",
    "test = pd.read_csv('./input/application_test.csv')\n",
    "print('data test Shapes : ', data.shape, test.shape)\n",
    "\n",
    "y = data['TARGET']\n",
    "del data['TARGET']\n",
    "gc.collect()\n",
    "all_data = pd.concat((data, test)).reset_index(drop=True)\n",
    "\n",
    "inc_by_org = all_data[['AMT_INCOME_TOTAL', 'ORGANIZATION_TYPE']].groupby('ORGANIZATION_TYPE').median()['AMT_INCOME_TOTAL']\n",
    "all_data['NEW_INC_BY_ORG'] = all_data['ORGANIZATION_TYPE'].map(inc_by_org)\n",
    "print('all_data Shape : ', all_data.shape)\n",
    "\n",
    "for bin_feature in ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n",
    "    all_data[bin_feature], uniques = pd.factorize(all_data[bin_feature])\n",
    "all_data, cat_cols = one_hot_encoder(all_data)\n",
    "print('all_data Shape : ', all_data.shape)\n",
    "# Some simple new features (percentages)\n",
    "#all_data['DAYS_EMPLOYED'].replace(365243, np.nan, inplace= True)\n",
    "n_data = data.shape[0]\n",
    "\n",
    "all_data['INCOME_DAYS_PERC'] = all_data['AMT_INCOME_TOTAL'] / all_data['DAYS_BIRTH']\n",
    "all_data['DAYS_EMPLOYED_PERC'] = all_data['DAYS_EMPLOYED'] / all_data['DAYS_BIRTH']\n",
    "all_data['INCOME_CREDIT_PERC'] = all_data['AMT_INCOME_TOTAL'] / all_data['AMT_CREDIT']\n",
    "all_data['INCOME_PER_PERSON'] = all_data['AMT_INCOME_TOTAL'] / all_data['CNT_FAM_MEMBERS']\n",
    "all_data['ANNUITY_INCOME_PERC'] = all_data['AMT_ANNUITY'] / all_data['AMT_INCOME_TOTAL']\n",
    "all_data['PAYMENT_RATE'] = all_data['AMT_CREDIT'] / all_data['AMT_ANNUITY']\n",
    "print('all_data Shape : ', all_data.shape)\n",
    "del data,test\n",
    "gc.collect()\n",
    "\n",
    "bureau = pd.read_csv('./input/bureau.csv')\n",
    "buro_bal = pd.read_csv('./input/bureau_balance.csv')\n",
    "print('bureau  buro_bal Shape : ', bureau.shape,buro_bal.shape)\n",
    "buro_bal, bb_cat = one_hot_encoder(buro_bal)\n",
    "bureau, bureau_cat = one_hot_encoder(bureau)\n",
    "bb_aggregations = {'MONTHS_BALANCE': ['min', 'max','mean']}\n",
    "for col in bb_cat:\n",
    "    bb_aggregations[col] = ['mean']\n",
    "    bb_agg = buro_bal.groupby('SK_ID_BUREAU').agg(bb_aggregations)\n",
    "bb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()])\n",
    "bb_agg['BB_COUNT'] = buro_bal.groupby('SK_ID_BUREAU').size()\n",
    "print('bb_agg Shape : ', bb_agg.shape)\n",
    "bureau = bureau.join(bb_agg, how='left', on='SK_ID_BUREAU')\n",
    "bureau.drop(columns= 'SK_ID_BUREAU', inplace= True)\n",
    "print('bureau Shape : ', bureau.shape)\n",
    "del buro_bal,bb_agg\n",
    "gc.collect()\n",
    "bureau['PEC_ANNUITY_DEBT'] = bureau['AMT_ANNUITY']/bureau['AMT_CREDIT_SUM_DEBT']\n",
    "num_aggregations = {\n",
    "    'DAYS_CREDIT': ['min', 'max', 'mean', 'var'],\n",
    "    'CREDIT_DAY_OVERDUE': ['max', 'mean'],\n",
    "    'DAYS_CREDIT_ENDDATE': ['min', 'max', 'mean'],\n",
    "    'AMT_CREDIT_MAX_OVERDUE': ['mean'],\n",
    "    'CNT_CREDIT_PROLONG': ['sum','mean'],\n",
    "    'PEC_ANNUITY_DEBT':['min','max','mean'],\n",
    "    'AMT_CREDIT_SUM': ['max','mean', 'sum'],\n",
    "    'AMT_CREDIT_SUM_DEBT': ['max','mean', 'sum'],\n",
    "    'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n",
    "    'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'],\n",
    "    'DAYS_CREDIT_UPDATE': ['min','max','mean'],\n",
    "    'AMT_ANNUITY': ['max', 'mean'],\n",
    "    'MONTHS_BALANCE_MIN': ['min'],\n",
    "    'MONTHS_BALANCE_MAX': ['max'],\n",
    "    'MONTHS_BALANCE_MEAN': ['mean'],\n",
    "    'BB_COUNT': ['mean', 'sum']\n",
    "}\n",
    "cat_aggregations = {}\n",
    "for cat in bureau_cat: cat_aggregations[cat] = ['mean']\n",
    "for cat in bb_cat: cat_aggregations[cat + \"_MEAN\"] = ['mean']\n",
    "bureau_agg = bureau.groupby('SK_ID_CURR').agg(dict(num_aggregations, **cat_aggregations))\n",
    "bureau_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\n",
    "bureau_agg['BURO_COUNT'] = bureau.groupby('SK_ID_CURR').size()\n",
    "bureau_agg['BURO_MAX_OVERDUE_DEBT'] = bureau_agg['BURO_AMT_CREDIT_MAX_OVERDUE_MEAN']/bureau_agg['BURO_AMT_CREDIT_SUM_DEBT_MEAN']\n",
    "print('bureau_agg Shape : ', bureau_agg.shape)\n",
    "\n",
    "active = bureau[bureau['CREDIT_ACTIVE_Active'] == 1]\n",
    "active_agg = active.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "active_agg.columns = pd.Index(['ACT_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n",
    "active_agg['ACT_COUNT'] = active.groupby('SK_ID_CURR').size()\n",
    "bureau_agg = bureau_agg.reset_index().join(active_agg, how='left', on='SK_ID_CURR')\n",
    "print('bureau_agg Shape : ', bureau_agg.shape)\n",
    "del active, active_agg\n",
    "gc.collect()\n",
    "\n",
    "closed = bureau[bureau['CREDIT_ACTIVE_Closed'] == 1]\n",
    "closed_agg = closed.groupby('SK_ID_CURR').agg(dict({'DAYS_ENDDATE_FACT': ['min','max','mean']},**num_aggregations))\n",
    "closed_agg.columns = pd.Index(['CLS_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n",
    "closed_agg['CLS_COUNT'] = closed.groupby('SK_ID_CURR').size()\n",
    "bureau_agg = bureau_agg.join(closed_agg, how='left', on='SK_ID_CURR')\n",
    "print('bureau_agg Shape : ', bureau_agg.shape)\n",
    "del closed, closed_agg, bureau\n",
    "gc.collect()\n",
    "\n",
    "all_data = all_data.merge(bureau_agg, how='left', on='SK_ID_CURR')\n",
    "print('all_data Shape : ', all_data.shape)\n",
    "del bureau_agg\n",
    "gc.collect()\n",
    "\n",
    "prev = pd.read_csv('./input/previous_application.csv')\n",
    "print('prev Shape : ', prev.shape)\n",
    "prev, cat_cols = one_hot_encoder(prev, nan_as_category= True)\n",
    "# Days 365.243 values -> nan\n",
    "prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace= True)\n",
    "prev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "prev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\n",
    "prev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "prev['DAYS_TERMINATION'].replace(365243, np.nan, inplace= True)\n",
    "# Add feature: value ask / value received percentage\n",
    "prev['APP_CREDIT_PERC'] = prev['AMT_APPLICATION'] / prev['AMT_CREDIT']\n",
    "prev['NEW_PA_AMT_UNREPAID'] = prev[prev['DAYS_LAST_DUE_1ST_VERSION'] > 0]\\\n",
    "    ['DAYS_LAST_DUE_1ST_VERSION']/30*(prev[prev['DAYS_LAST_DUE_1ST_VERSION'] > 0]['AMT_ANNUITY'])\n",
    "# Previous applications numeric features\n",
    "num_aggregations = {\n",
    "    'AMT_ANNUITY': ['min','max', 'mean'],\n",
    "    'AMT_APPLICATION': ['min','max', 'mean'],\n",
    "    'AMT_CREDIT': ['min','max', 'mean'],\n",
    "    'APP_CREDIT_PERC': ['min', 'max', 'mean', 'var'],\n",
    "    'AMT_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "    'AMT_GOODS_PRICE': ['min', 'max', 'mean'],\n",
    "    'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean'],\n",
    "    'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "    'DAYS_DECISION': ['min', 'max', 'mean'],\n",
    "    'SELLERPLACE_AREA': ['min', 'max', 'mean'],\n",
    "    'DAYS_FIRST_DRAWING': ['min', 'max', 'mean'],\n",
    "    'DAYS_FIRST_DUE': ['min', 'max', 'mean'],\n",
    "    'DAYS_LAST_DUE_1ST_VERSION': ['min', 'max', 'mean'],\n",
    "    'DAYS_LAST_DUE': ['min', 'max', 'mean'],\n",
    "    'DAYS_TERMINATION': ['min', 'max', 'mean'],\n",
    "    'CNT_PAYMENT': ['mean', 'sum'],\n",
    "    'NEW_PA_AMT_UNREPAID':['min','max','mean', 'sum']\n",
    "}\n",
    "# Previous applications categorical features\n",
    "cat_aggregations = {}\n",
    "for cat in cat_cols:\n",
    "    cat_aggregations[cat] = ['mean']\n",
    "\n",
    "prev_agg = prev.groupby('SK_ID_CURR').agg(dict(num_aggregations, **cat_aggregations))\n",
    "prev_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\n",
    "prev_agg['PREV_COUNT'] = prev.groupby('SK_ID_CURR').size()\n",
    "print('prev_agg Shape : ', prev_agg.shape)\n",
    "# Previous Applications: Approved Applications - only numerical features\n",
    "\n",
    "ap_num_aggregations = {\n",
    "        'AMT_ANNUITY': ['min','max', 'mean'],\n",
    "        'AMT_APPLICATION': ['min','max', 'mean'],\n",
    "        'AMT_CREDIT': ['min','max', 'mean'],\n",
    "        'APP_CREDIT_PERC': ['min', 'max', 'mean', 'var'],\n",
    "        'AMT_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'AMT_GOODS_PRICE': ['min', 'max', 'mean'],\n",
    "        'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean'],\n",
    "        'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'DAYS_DECISION': ['min', 'max', 'mean'],\n",
    "        'SELLERPLACE_AREA': ['min', 'max', 'mean'],\n",
    "        'DAYS_FIRST_DRAWING': ['min', 'max', 'mean'],\n",
    "        'DAYS_FIRST_DUE': ['min', 'max', 'mean'],\n",
    "        'DAYS_LAST_DUE_1ST_VERSION': ['min', 'max', 'mean'],\n",
    "        'DAYS_LAST_DUE': ['min', 'max', 'mean'],\n",
    "        'DAYS_TERMINATION': ['min', 'max', 'mean'],\n",
    "        'CNT_PAYMENT': ['mean', 'sum']\n",
    "    }\n",
    "approved = prev[prev['NAME_CONTRACT_STATUS_Approved'] == 1]\n",
    "approved_agg = approved.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "approved_agg.columns = pd.Index(['APR_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\n",
    "approved_agg['APR_COUNT'] = approved.groupby('SK_ID_CURR').size()\n",
    "prev_agg = prev_agg.reset_index().join(approved_agg, how='left', on='SK_ID_CURR')\n",
    "print('prev_agg Shape : ', prev_agg.shape)\n",
    "# Previous Applications: Refused Applications - only numerical features\n",
    "refused = prev[prev['NAME_CONTRACT_STATUS_Refused'] == 1]\n",
    "refused_agg = refused.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "refused_agg.columns = pd.Index(['REF_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\n",
    "refused_agg['REF_COUNT'] = refused.groupby('SK_ID_CURR').size()\n",
    "prev_agg = prev_agg.join(refused_agg, how='left', on='SK_ID_CURR')\n",
    "print('prev_agg Shape : ', prev_agg.shape)\n",
    "del refused, refused_agg, approved, approved_agg, prev\n",
    "gc.collect()\n",
    "\n",
    "all_data = all_data.merge(prev_agg, how='left', on='SK_ID_CURR')\n",
    "print('all_data Shape : ', all_data.shape)\n",
    "del prev_agg\n",
    "gc.collect()\n",
    "\n",
    "pos = pd.read_csv('./input/POS_CASH_balance.csv')\n",
    "print('pos Shape : ', pos.shape)\n",
    "pos, cat_cols = one_hot_encoder(pos, nan_as_category= True)\n",
    "# Features\n",
    "aggregations = {\n",
    "    'MONTHS_BALANCE': ['min','max', 'mean'],\n",
    "    'CNT_INSTALMENT': ['sum', 'mean'],\n",
    "    'SK_DPD': ['sum','max','min','mean','size'],\n",
    "    'SK_DPD_DEF': ['sum','max', 'min','mean','size']\n",
    "}\n",
    "for cat in cat_cols:\n",
    "    aggregations[cat] = ['mean']\n",
    "\n",
    "pos_agg = pos.groupby('SK_ID_CURR').agg(aggregations)\n",
    "pos_agg.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()])\n",
    "# Count pos cash accounts\n",
    "pos_agg['POS_COUNT'] = pos.groupby('SK_ID_CURR').size()\n",
    "print('pos_agg Shape : ', pos_agg.shape)\n",
    "del pos\n",
    "gc.collect()\n",
    "\n",
    "all_data = all_data.join(pos_agg, how='left', on='SK_ID_CURR')\n",
    "print('all_data Shape : ', all_data.shape)\n",
    "del pos_agg\n",
    "gc.collect()\n",
    "\n",
    "ins = pd.read_csv('./input/installments_payments.csv')\n",
    "print('ins Shape : ', ins.shape)\n",
    "ins, cat_cols = one_hot_encoder(ins, nan_as_category= True)\n",
    "# Percentage and difference paid in each installment (amount paid and installment value)\n",
    "ins['PAYMENT_PERC'] = ins['AMT_PAYMENT'] / ins['AMT_INSTALMENT']\n",
    "ins['PAYMENT_DIFF'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT']\n",
    "# Days past due and days before due (no negative values)\n",
    "ins['DPD'] = ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT']\n",
    "ins['NEW_IP_DPD_RATIO'] = ins['DAYS_ENTRY_PAYMENT'] / ins['DAYS_INSTALMENT']\n",
    "ins['DBD'] = ins['DAYS_INSTALMENT'] - ins['DAYS_ENTRY_PAYMENT']\n",
    "ins['NEW_IP_DBD_RATIO'] = ins['DAYS_INSTALMENT'] / ins['DAYS_ENTRY_PAYMENT']\n",
    "ins['DPD'] = ins['DPD'].apply(lambda x: x if x > 0 else 0)\n",
    "ins['DBD'] = ins['DBD'].apply(lambda x: x if x > 0 else 0)\n",
    "# Features: Perform aggregations\n",
    "aggregations = {\n",
    "    'NUM_INSTALMENT_VERSION': ['nunique'],\n",
    "    'DPD': ['max','mean', 'sum'],\n",
    "    'NEW_IP_DPD_RATIO':['max', 'mean','min'],\n",
    "    'NEW_IP_DBD_RATIO':['max', 'mean','min'],\n",
    "    'DBD': ['max', 'mean', 'sum'],\n",
    "    'PAYMENT_PERC': ['max', 'mean', 'sum', 'var'],\n",
    "    'PAYMENT_DIFF': ['max', 'mean', 'sum', 'var'],\n",
    "    'AMT_INSTALMENT': ['max', 'mean', 'sum'],\n",
    "    'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n",
    "    'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum'],\n",
    "    'DAYS_INSTALMENT': ['max', 'mean', 'sum']\n",
    "}\n",
    "for cat in cat_cols:\n",
    "    aggregations[cat] = ['mean']\n",
    "ins_agg = ins.groupby('SK_ID_CURR').agg(aggregations)\n",
    "ins_agg.columns = pd.Index(['INS_' + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\n",
    "# Count installments accounts\n",
    "ins_agg['INS_COUNT'] = ins.groupby('SK_ID_CURR').size()\n",
    "print('ins_agg Shape : ', ins_agg.shape)\n",
    "del ins\n",
    "gc.collect()\n",
    "\n",
    "all_data = all_data.join(ins_agg, how='left', on='SK_ID_CURR')\n",
    "print('all_data Shape : ', all_data.shape)\n",
    "del ins_agg\n",
    "gc.collect()\n",
    "\n",
    "cc = pd.read_csv('./input/credit_card_balance.csv')\n",
    "print('cc Shape : ', cc.shape)\n",
    "cc, cat_cols = one_hot_encoder(cc, nan_as_category= True)\n",
    "# Features: Perform aggregations\n",
    "num_aggregations = {\n",
    "    'MONTHS_BALANCE': ['min','max', 'mean'],\n",
    "    'AMT_BALANCE': ['min','max', 'mean','sum'],\n",
    "    'AMT_CREDIT_LIMIT_ACTUAL': ['min','max', 'mean','sum','var'],\n",
    "    'AMT_DRAWINGS_ATM_CURRENT': ['min','max', 'mean','sum'],\n",
    "    'AMT_DRAWINGS_CURRENT': ['min','max', 'mean','sum'],\n",
    "    'AMT_DRAWINGS_OTHER_CURRENT': ['min','max', 'mean','sum'],\n",
    "    'AMT_DRAWINGS_POS_CURRENT': ['min','max', 'mean','sum'],\n",
    "    'AMT_INST_MIN_REGULARITY': ['min','max', 'mean','sum','var'],\n",
    "    'AMT_PAYMENT_CURRENT':['min','max', 'mean','sum'],\n",
    "    'AMT_PAYMENT_TOTAL_CURRENT':['min','max', 'mean','sum'],\n",
    "    'AMT_RECEIVABLE_PRINCIPAL':['min','max', 'mean','sum','var'],\n",
    "    'AMT_RECIVABLE':['min','max', 'mean','sum','var'],\n",
    "    'AMT_TOTAL_RECEIVABLE':['min','max', 'mean','sum','var'],\n",
    "    'CNT_DRAWINGS_ATM_CURRENT':['mean','sum'],\n",
    "    'CNT_DRAWINGS_CURRENT':['mean','sum'],\n",
    "    'CNT_DRAWINGS_OTHER_CURRENT':['mean','sum'],\n",
    "    'CNT_DRAWINGS_POS_CURRENT':['mean','sum'],\n",
    "    'CNT_INSTALMENT_MATURE_CUM':['mean','sum'],\n",
    "    'SK_DPD':['max', 'mean', 'sum'],\n",
    "    'SK_DPD_DEF':['max', 'mean', 'sum']\n",
    "}\n",
    "cat_aggregations = {}\n",
    "for cat in cat_cols:\n",
    "    cat_aggregations[cat] = ['mean']\n",
    "# General aggregations\n",
    "cc_agg = cc.groupby('SK_ID_CURR').agg(dict(num_aggregations,**cat_aggregations))\n",
    "cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n",
    "# Count credit card lines\n",
    "cc_agg['CC_COUNT'] = cc.groupby('SK_ID_CURR').size()\n",
    "\n",
    "#cc_agg['INSTALLMENTS_PER_LOAN'] = cc_agg['CNT_INSTALMENT_MATURE_CUM_SUM'] / cc_agg['CC_COUNT']\n",
    "del cc\n",
    "print('cc_agg Shape : ', cc_agg.shape)\n",
    "gc.collect()\n",
    "\n",
    "all_data = all_data.join(cc_agg, how='left', on='SK_ID_CURR')\n",
    "print('all_data Shape : ', all_data.shape)\n",
    "del cc_agg\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(307511, 846)\n",
      "(48744, 846)\n"
     ]
    }
   ],
   "source": [
    "all_data_na = (all_data.isnull().sum() / len(all_data)) * 100\n",
    "all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\n",
    "missing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\n",
    "\n",
    "drop_col = missing_data[missing_data['Missing Ratio']==100]\n",
    "missing_data = missing_data[missing_data['Missing Ratio']<100]\n",
    "\n",
    "all_data.drop(drop_col.index.values,1,inplace=True)\n",
    "ID = all_data.SK_ID_CURR\n",
    "all_data.drop('SK_ID_CURR',1,inplace=True)\n",
    "all_data = all_data.fillna(all_data.mean()).clip(-1e11,1e11)\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(all_data)\n",
    "data = scaler.transform(all_data[:n_data])\n",
    "test = scaler.transform(all_data[n_data:])\n",
    "#del avg_buro, avg_prev,avg_pos,avg_cc_bal,avg_inst\n",
    "del all_data\n",
    "gc.collect()\n",
    "print (data.shape)\n",
    "print (test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "NFOLDS=5\n",
    "SEED = 1700\n",
    "folds = KFold(n_splits=NFOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "def get_nn_oof(x_train, y_train, x_test):\n",
    "    oof_train = np.zeros((x_train.shape[0],))\n",
    "    oof_test = np.zeros((x_test.shape[0],))\n",
    "    oof_test_skf = np.empty((NFOLDS, x_test.shape[0]))\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(folds.split(x_train)):\n",
    "        x_tr = x_train[train_index]\n",
    "        y_tr = y_train.iloc[train_index]\n",
    "        x_te = x_train[test_index]\n",
    "        y_te = y_train.iloc[test_index]\n",
    "        print( 'Setting up neural network...' )\n",
    "        K.clear_session()\n",
    "        nn = Sequential()\n",
    "        nn.add(Dense(units = 800 , kernel_initializer = 'normal', input_dim = 846))\n",
    "        nn.add(PReLU())\n",
    "        #nn.add(Dropout(.3))\n",
    "        nn.add(Dense(units = 320 , kernel_initializer = 'normal'))\n",
    "        nn.add(PReLU())\n",
    "        nn.add(BatchNormalization())\n",
    "        #nn.add(Dropout(.3))\n",
    "        nn.add(Dense(units = 128 , kernel_initializer = 'normal'))\n",
    "        nn.add(PReLU())\n",
    "        nn.add(BatchNormalization())\n",
    "        #nn.add(Dropout(.3))\n",
    "        nn.add(Dense(units = 32, kernel_initializer = 'normal'))\n",
    "        nn.add(PReLU())\n",
    "        nn.add(BatchNormalization())\n",
    "        #nn.add(Dropout(.3))\n",
    "        nn.add(Dense(units = 16, kernel_initializer = 'normal'))\n",
    "        nn.add(PReLU())\n",
    "        nn.add(BatchNormalization())\n",
    "        #nn.add(Dropout(.3))\n",
    "        nn.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "        nn.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "        early_stopping = EarlyStopping(patience=5, verbose=1)\n",
    "        model_checkpoint = ModelCheckpoint(\"./keras.model\", save_best_only=True, verbose=1)\n",
    "        reduce_lr = ReduceLROnPlateau(factor=0.1, patience=3, min_lr=0.00001, verbose=1)\n",
    "        print( 'Fitting neural network...' )\n",
    "        nn.fit(x_tr, y_tr, validation_data = (x_te, y_te), epochs=100,batch_size=512,\n",
    "              callbacks=[early_stopping, model_checkpoint, reduce_lr])\n",
    "        nn = load_model(\"./keras.model\")\n",
    "        print( 'Predicting...' )\n",
    "        oof_train[test_index] = nn.predict(x_te).flatten().clip(0,1)\n",
    "        oof_test_skf[i, :] += nn.predict(x_test).flatten().clip(0,1) / folds.n_splits\n",
    "\n",
    "        print('Fold %2d Train AUC : %.6f' % (i + 1, roc_auc_score(y_tr, nn.predict(x_tr).flatten().clip(0,1))))\n",
    "        print('Fold %2d Test AUC : %.6f' % (i + 1, roc_auc_score(y_te, oof_train[test_index])))\n",
    "        del nn,x_tr,y_tr,x_te,y_te\n",
    "        gc.collect()\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up neural network...\n",
      "Fitting neural network...\n",
      "Train on 246008 samples, validate on 61503 samples\n",
      "Epoch 1/100\n",
      "246008/246008 [==============================] - 4s 18us/step - loss: 0.3872 - val_loss: 0.2505\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.25050, saving model to ./keras.model\n",
      "Epoch 2/100\n",
      "246008/246008 [==============================] - 4s 14us/step - loss: 0.2463 - val_loss: 0.2544\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.25050\n",
      "Epoch 3/100\n",
      "246008/246008 [==============================] - 4s 14us/step - loss: 0.2443 - val_loss: 0.2481\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.25050 to 0.24810, saving model to ./keras.model\n",
      "Epoch 4/100\n",
      "246008/246008 [==============================] - 4s 14us/step - loss: 0.2435 - val_loss: 0.2517\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.24810\n",
      "Epoch 5/100\n",
      "246008/246008 [==============================] - 4s 14us/step - loss: 0.2424 - val_loss: 0.2455\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.24810 to 0.24551, saving model to ./keras.model\n",
      "Epoch 6/100\n",
      "246008/246008 [==============================] - 4s 14us/step - loss: 0.2416 - val_loss: 0.2688\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.24551\n",
      "Epoch 7/100\n",
      "246008/246008 [==============================] - 4s 14us/step - loss: 0.2409 - val_loss: 0.2465\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.24551\n",
      "Epoch 8/100\n",
      "246008/246008 [==============================] - 4s 14us/step - loss: 0.2403 - val_loss: 0.2498\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.24551\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 9/100\n",
      "246008/246008 [==============================] - 4s 14us/step - loss: 0.2371 - val_loss: 0.2411\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.24551 to 0.24108, saving model to ./keras.model\n",
      "Epoch 10/100\n",
      "246008/246008 [==============================] - 4s 14us/step - loss: 0.2359 - val_loss: 0.2419\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.24108\n",
      "Epoch 11/100\n",
      "246008/246008 [==============================] - 4s 14us/step - loss: 0.2355 - val_loss: 0.2422\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.24108\n",
      "Epoch 12/100\n",
      "246008/246008 [==============================] - 4s 14us/step - loss: 0.2350 - val_loss: 0.2426\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.24108\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 13/100\n",
      "246008/246008 [==============================] - 4s 14us/step - loss: 0.2343 - val_loss: 0.2423\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.24108\n",
      "Epoch 14/100\n",
      "246008/246008 [==============================] - 4s 14us/step - loss: 0.2342 - val_loss: 0.2424\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.24108\n",
      "Epoch 00014: early stopping\n",
      "Predicting...\n",
      "Fold  1 Train AUC : 0.793525\n",
      "Fold  1 Test AUC : 0.769655\n",
      "Setting up neural network...\n",
      "Fitting neural network...\n",
      "Train on 246009 samples, validate on 61502 samples\n",
      "Epoch 1/100\n",
      "246009/246009 [==============================] - 4s 18us/step - loss: 0.3865 - val_loss: 0.2602\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.26017, saving model to ./keras.model\n",
      "Epoch 2/100\n",
      "246009/246009 [==============================] - 4s 14us/step - loss: 0.2451 - val_loss: 0.2513\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.26017 to 0.25130, saving model to ./keras.model\n",
      "Epoch 3/100\n",
      "246009/246009 [==============================] - 4s 14us/step - loss: 0.2436 - val_loss: 0.2601\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.25130\n",
      "Epoch 4/100\n",
      "246009/246009 [==============================] - 4s 14us/step - loss: 0.2426 - val_loss: 0.2729\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.25130\n",
      "Epoch 5/100\n",
      "246009/246009 [==============================] - 4s 14us/step - loss: 0.2419 - val_loss: 0.2632\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.25130\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 6/100\n",
      "246009/246009 [==============================] - 4s 14us/step - loss: 0.2392 - val_loss: 0.2439\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.25130 to 0.24391, saving model to ./keras.model\n",
      "Epoch 7/100\n",
      "246009/246009 [==============================] - 4s 14us/step - loss: 0.2379 - val_loss: 0.2436\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.24391 to 0.24364, saving model to ./keras.model\n",
      "Epoch 8/100\n",
      "246009/246009 [==============================] - 4s 15us/step - loss: 0.2373 - val_loss: 0.2440\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.24364\n",
      "Epoch 9/100\n",
      "246009/246009 [==============================] - 4s 14us/step - loss: 0.2367 - val_loss: 0.2445\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.24364\n",
      "Epoch 10/100\n",
      "246009/246009 [==============================] - 4s 14us/step - loss: 0.2362 - val_loss: 0.2441\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.24364\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 11/100\n",
      "246009/246009 [==============================] - 4s 14us/step - loss: 0.2350 - val_loss: 0.2439\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.24364\n",
      "Epoch 12/100\n",
      "246009/246009 [==============================] - 4s 14us/step - loss: 0.2349 - val_loss: 0.2440\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.24364\n",
      "Epoch 00012: early stopping\n",
      "Predicting...\n",
      "Fold  2 Train AUC : 0.787018\n",
      "Fold  2 Test AUC : 0.773853\n",
      "Setting up neural network...\n",
      "Fitting neural network...\n",
      "Train on 246009 samples, validate on 61502 samples\n",
      "Epoch 1/100\n",
      "246009/246009 [==============================] - 4s 18us/step - loss: 0.3834 - val_loss: 0.2517\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.25172, saving model to ./keras.model\n",
      "Epoch 2/100\n",
      "246009/246009 [==============================] - 4s 14us/step - loss: 0.2456 - val_loss: 0.2508\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.25172 to 0.25079, saving model to ./keras.model\n",
      "Epoch 3/100\n",
      "246009/246009 [==============================] - 4s 14us/step - loss: 0.2436 - val_loss: 0.2636\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.25079\n",
      "Epoch 4/100\n",
      "246009/246009 [==============================] - 4s 14us/step - loss: 0.2428 - val_loss: 0.2584\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.25079\n",
      "Epoch 5/100\n",
      "246009/246009 [==============================] - 4s 14us/step - loss: 0.2418 - val_loss: 0.2651\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.25079\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 6/100\n",
      "246009/246009 [==============================] - 4s 14us/step - loss: 0.2389 - val_loss: 0.2425\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.25079 to 0.24252, saving model to ./keras.model\n",
      "Epoch 7/100\n",
      "246009/246009 [==============================] - 4s 14us/step - loss: 0.2376 - val_loss: 0.2432\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.24252\n",
      "Epoch 8/100\n",
      "246009/246009 [==============================] - 4s 15us/step - loss: 0.2370 - val_loss: 0.2432\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.24252\n",
      "Epoch 9/100\n",
      "246009/246009 [==============================] - 4s 14us/step - loss: 0.2365 - val_loss: 0.2432\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.24252\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 10/100\n",
      "246009/246009 [==============================] - 4s 15us/step - loss: 0.2353 - val_loss: 0.2433\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.24252\n",
      "Epoch 11/100\n",
      "246009/246009 [==============================] - 4s 14us/step - loss: 0.2351 - val_loss: 0.2434\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.24252\n",
      "Epoch 00011: early stopping\n",
      "Predicting...\n",
      "Fold  3 Train AUC : 0.787498\n",
      "Fold  3 Test AUC : 0.770431\n",
      "Setting up neural network...\n",
      "Fitting neural network...\n",
      "Train on 246009 samples, validate on 61502 samples\n",
      "Epoch 1/100\n",
      "246009/246009 [==============================] - 4s 18us/step - loss: 0.3860 - val_loss: 0.2889\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.28889, saving model to ./keras.model\n",
      "Epoch 2/100\n",
      "246009/246009 [==============================] - 4s 14us/step - loss: 0.2458 - val_loss: 0.2452\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.28889 to 0.24523, saving model to ./keras.model\n",
      "Epoch 3/100\n",
      "246009/246009 [==============================] - 4s 14us/step - loss: 0.2440 - val_loss: 0.2435\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.24523 to 0.24354, saving model to ./keras.model\n",
      "Epoch 4/100\n",
      "246009/246009 [==============================] - 4s 14us/step - loss: 0.2432 - val_loss: 0.2449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00004: val_loss did not improve from 0.24354\n",
      "Epoch 5/100\n",
      "246009/246009 [==============================] - 3s 14us/step - loss: 0.2423 - val_loss: 0.2758\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.24354\n",
      "Epoch 6/100\n",
      "246009/246009 [==============================] - 3s 14us/step - loss: 0.2418 - val_loss: 0.2590\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.24354\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 7/100\n",
      "246009/246009 [==============================] - 3s 14us/step - loss: 0.2390 - val_loss: 0.2425\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.24354 to 0.24249, saving model to ./keras.model\n",
      "Epoch 8/100\n",
      "246009/246009 [==============================] - 3s 14us/step - loss: 0.2380 - val_loss: 0.2416\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.24249 to 0.24158, saving model to ./keras.model\n",
      "Epoch 9/100\n",
      "246009/246009 [==============================] - 3s 14us/step - loss: 0.2374 - val_loss: 0.2428\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.24158\n",
      "Epoch 10/100\n",
      "246009/246009 [==============================] - 4s 14us/step - loss: 0.2368 - val_loss: 0.2420\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.24158\n",
      "Epoch 11/100\n",
      "246009/246009 [==============================] - 4s 15us/step - loss: 0.2364 - val_loss: 0.2432\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.24158\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 12/100\n",
      "246009/246009 [==============================] - 4s 15us/step - loss: 0.2355 - val_loss: 0.2424\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.24158\n",
      "Epoch 13/100\n",
      "246009/246009 [==============================] - 3s 14us/step - loss: 0.2352 - val_loss: 0.2425\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.24158\n",
      "Epoch 00013: early stopping\n",
      "Predicting...\n",
      "Fold  4 Train AUC : 0.789800\n",
      "Fold  4 Test AUC : 0.770531\n",
      "Setting up neural network...\n",
      "Fitting neural network...\n",
      "Train on 246009 samples, validate on 61502 samples\n",
      "Epoch 1/100\n",
      "246009/246009 [==============================] - 4s 18us/step - loss: 0.3841 - val_loss: 0.2600\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.26002, saving model to ./keras.model\n",
      "Epoch 2/100\n",
      "246009/246009 [==============================] - 4s 14us/step - loss: 0.2446 - val_loss: 0.2482\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.26002 to 0.24820, saving model to ./keras.model\n",
      "Epoch 3/100\n",
      "246009/246009 [==============================] - 4s 14us/step - loss: 0.2431 - val_loss: 0.2476\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.24820 to 0.24762, saving model to ./keras.model\n",
      "Epoch 4/100\n",
      "246009/246009 [==============================] - 4s 14us/step - loss: 0.2420 - val_loss: 0.2623\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.24762\n",
      "Epoch 5/100\n",
      "246009/246009 [==============================] - 4s 14us/step - loss: 0.2414 - val_loss: 0.2581\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.24762\n",
      "Epoch 6/100\n",
      "246009/246009 [==============================] - 4s 14us/step - loss: 0.2406 - val_loss: 0.2509\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.24762\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 7/100\n",
      "246009/246009 [==============================] - 4s 14us/step - loss: 0.2379 - val_loss: 0.2460\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.24762 to 0.24595, saving model to ./keras.model\n",
      "Epoch 8/100\n",
      "246009/246009 [==============================] - 4s 14us/step - loss: 0.2369 - val_loss: 0.2459\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.24595 to 0.24589, saving model to ./keras.model\n",
      "Epoch 9/100\n",
      "246009/246009 [==============================] - 4s 15us/step - loss: 0.2363 - val_loss: 0.2454\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.24589 to 0.24545, saving model to ./keras.model\n",
      "Epoch 10/100\n",
      "246009/246009 [==============================] - 4s 14us/step - loss: 0.2360 - val_loss: 0.2455\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.24545\n",
      "Epoch 11/100\n",
      "246009/246009 [==============================] - 4s 14us/step - loss: 0.2356 - val_loss: 0.2472\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.24545\n",
      "Epoch 12/100\n",
      "246009/246009 [==============================] - 4s 14us/step - loss: 0.2351 - val_loss: 0.2461\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.24545\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 13/100\n",
      "246009/246009 [==============================] - 4s 14us/step - loss: 0.2341 - val_loss: 0.2461\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.24545\n",
      "Epoch 14/100\n",
      "246009/246009 [==============================] - 4s 14us/step - loss: 0.2340 - val_loss: 0.2462\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.24545\n",
      "Epoch 00014: early stopping\n",
      "Predicting...\n",
      "Fold  5 Train AUC : 0.791920\n",
      "Fold  5 Test AUC : 0.768271\n"
     ]
    }
   ],
   "source": [
    "oof_train, oof_test = get_nn_oof(data, y, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_train_csv = pd.DataFrame()\n",
    "oof_train_csv['ID'] = ID[:n_data]\n",
    "oof_train_csv['nn_oof_train'] = oof_train\n",
    "\n",
    "oof_train_csv.to_csv('./output/nn_oof_train.csv', index=False)\n",
    "\n",
    "oof_test_csv = pd.DataFrame()\n",
    "oof_test_csv['ID'] = ID[n_data:]\n",
    "oof_test_csv['nn_oof_test'] = oof_test\n",
    "\n",
    "oof_test_csv.to_csv('./output/nn_oof_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['SK_ID_CURR'] = ID[n_data:]\n",
    "submission['TARGET'] = oof_test\n",
    "submission.to_csv('nn_predict.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
